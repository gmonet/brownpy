{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import h5py\r\n",
    "from netCDF4 import Dataset\r\n",
    "import numpy as np\r\n",
    "import cupy as cp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "size=(8*1024,2,10_000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "array = cp.random.normal(size=size, dtype=cp.float32).get()\r\n",
    "print(f'{array.nbytes/1024**3:.2f} GiB')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Netcdf\r\n",
    "def write_nc(array):\r\n",
    "    with Dataset('test.nc', 'w', format=\"NETCDF4\") as rootgrp:\r\n",
    "        rootgrp.createDimension(\"idx\", array.shape[0])\r\n",
    "        rootgrp.createDimension(\"dim\", array.shape[1])\r\n",
    "        rootgrp.createDimension(\"step\", array.shape[2])\r\n",
    "        traj = rootgrp.createVariable(\"traj\", \"f4\", (\"idx\", \"dim\", \"step\"))\r\n",
    "        traj[:]=array\r\n",
    "%timeit write_nc(array)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#h5py\r\n",
    "def write_hdf5(array):\r\n",
    "    with h5py.File('test.hdf5', 'w') as f:\r\n",
    "        dset = f.create_dataset(\"mydataset\", data=array)\r\n",
    "%timeit write_hdf5(array)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#h5py chunck\r\n",
    "def write_hdf5_1(array, chunk_size=1000):\r\n",
    "    with h5py.File('test_chunck.hdf5', 'w') as f:\r\n",
    "        dset = f.create_dataset(\"mydataset\", shape=array.shape, dtype=array.dtype,\r\n",
    "                                chunks=(array.shape[0], array.shape[1], chunk_size))\r\n",
    "        for i, s in enumerate(dset.iter_chunks()):\r\n",
    "            dset[:,:,i*chunk_size:(i+1)*chunk_size] = array[:,:,i*chunk_size:(i+1)*chunk_size]\r\n",
    "\r\n",
    "def write_hdf5_2(array, chunk_size=1000):\r\n",
    "    with h5py.File('test_chunck.hdf5', 'w') as f:\r\n",
    "        dset = f.create_dataset(\"mydataset\", shape = (array.shape[0], array.shape[1], chunk_size),\r\n",
    "                                             maxshape=array.shape,\r\n",
    "                                             dtype=array.dtype)\r\n",
    "        i=0\r\n",
    "        dset[:,:,i:i+chunk_size] = array[:,:,i:i+chunk_size]\r\n",
    "        for i in range(chunk_size, array.shape[2], chunk_size):\r\n",
    "            dset.resize(dset.shape[2]+chunk_size, axis=2)\r\n",
    "            dset[:,:,i:i+chunk_size] = array[:,:,i:i+chunk_size]\r\n",
    "\r\n",
    "def write_hdf5_3(array, chunk_size=1000):\r\n",
    "    with h5py.File('test_chunck.hdf5', 'w') as f:\r\n",
    "        dset = f.create_dataset(\"mydataset\", shape=array.shape,\r\n",
    "                                             dtype=array.dtype,\r\n",
    "                                             chunks = (array.shape[0], array.shape[1], chunk_size))\r\n",
    "        for i in range(0, array.shape[2], chunk_size):\r\n",
    "            dset[:,:,i:i+chunk_size] = array[:,:,i:i+chunk_size]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%timeit write_hdf5_1(array)\r\n",
    "with h5py.File('test_chunck.hdf5', 'r') as f:\r\n",
    "    array_read = f['mydataset'][...]\r\n",
    "    assert np.all(array_read==array)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%timeit write_hdf5_2(array)\r\n",
    "with h5py.File('test_chunck.hdf5', 'r') as f:\r\n",
    "    array_read = f['mydataset'][...]\r\n",
    "    assert np.all(array_read==array)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%timeit write_hdf5_3(array)\r\n",
    "with h5py.File('test_chunck.hdf5', 'r') as f:\r\n",
    "    array_read = f['mydataset'][...]\r\n",
    "    assert np.all(array_read==array)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "f = h5py.File('test.hdf5', 'w')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "f.create_group('a')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 group \"/a\" (0 members)>"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "d4ce2f0b9c719736e7f661a13fe0b346d5bd093966a3397b765b95efddff6c19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}